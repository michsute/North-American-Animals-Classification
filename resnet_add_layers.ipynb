{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff4ef17-e6c9-4644-84ce-75b3f59bbcb1",
   "metadata": {},
   "source": [
    "# Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69109c9b-2717-4087-9761-16b6725bb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029cc2d-c7e5-4746-b642-838eb5d74f0c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816cf8e3-4d38-461c-a622-354da960c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import datetime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733dc5e4-8846-4cc7-97c2-9bca12b697d7",
   "metadata": {},
   "source": [
    "# Image loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b162d-9096-4068-a4f4-db73368c5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert images to PyTorch tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Values of ImageNet\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee7908-843e-4190-8dc5-fcdc0cababae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image\n",
    "img = Image.open('/exchange/dspro01/group3/data/train/bobcat/2015_Unit164_Ivan026_img0171.jpg')\n",
    "img_small = img.resize((512, 512))\n",
    "img_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7380a9-17cc-4cec-af21-10167ae65fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformed = transform(img)\n",
    "\n",
    "def denormalize(tensor):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "denormalized_image = to_pil(denormalize(img_transformed.clone()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7e497-97bd-49e8-8d0c-e8611041d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bff32b-34da-4c84-960d-c9f135ce5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility of Transformation\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece6847-5193-4995-b4be-3d2a3f82f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/exchange/dspro01/group3/data/train' \n",
    "# dataset = datasets.ImageFolder(root=data_dir, transform=transform) # assigns labels automatically according to folder's name\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform) # assigns labels automatically according to folder's name\n",
    "# Normalization independent of dataset so applied at root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a0127-f0a1-4a79-9509-bf79f04d41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 12  # number of categories\n",
    "num_epochs =  25  \n",
    "learning_rate = 0.001 \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88491ef9-29dc-497e-89d6-f957bc641a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)   \n",
    "val_size = int(0.2 * total_size)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21d8ec-25fe-4813-9b5f-c06ae24da5d5",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03f7ea-3e64-4785-8552-036e7c4f11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(seed) # For reproducibility\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1eb9e-ea1a-4530-a85b-d6b1db65757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset) == (2400 * 12 * 0.8) # Safety check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca98eb-9bb2-4a46-9d82-8ec7310121de",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f1dd8-906e-4dd5-8073-261419a2ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divides into batches, shuffles data, load in parallel\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61f616-3f8c-411d-97a1-4afe8eeee954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders dictionary\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a68e5e-f73a-4404-9421-eed3014d651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset_sizes dictionary for calculating losses and accuracies\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f367360-b7fe-4eff-a31d-0ee9d657e90b",
   "metadata": {},
   "source": [
    "# Model loading and transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af834b-0a20-473a-83f1-f023c439d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec779f0b-2bfd-46d7-b574-69b80d6cbebc",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb176e3-e5cc-4f1c-b08a-772127de9fd2",
   "metadata": {},
   "source": [
    "# Training & Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b864939-8d23-48fe-b71e-f8a4c003d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, recall_score, precision_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "input_features = model.fc.in_features\n",
    "hidden_dim1 = 512\n",
    "hidden_dim2 = 256\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(input_features, hidden_dim1),  # First layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(hidden_dim1, hidden_dim2),    # Second layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(hidden_dim2, num_classes)     # Classification layer\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f590654-29af-43e8-a14c-da5ef273977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_labels, val_predictions = [], []\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = f'best_model_{current_time}.pth'\n",
    "val_f1_scores, val_conf_matrices = [], []\n",
    "val_recall = []\n",
    "val_precision = []\n",
    "\n",
    "patience, early_stopping_counter, best_epoch = 5, 0, 0\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if phase == 'train':\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_accuracies.append(epoch_acc.item())\n",
    "        else:\n",
    "            val_losses.append(epoch_loss)\n",
    "            val_accuracies.append(epoch_acc.item())\n",
    "            val_labels.append(all_labels)\n",
    "            val_predictions.append(all_preds)\n",
    "\n",
    "           # Calculate F1-Score, and confusion matrix for validation phase\n",
    "            f1 = f1_score(all_labels, all_preds, average='weighted') # from sklearn\n",
    "            # 'weighted' in case of inbalance dataset or multiclass. Safety here.\n",
    "            recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "            precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "            \n",
    "            val_f1_scores.append(f1)\n",
    "            val_recall.append(recall)\n",
    "            val_precision.append(precision)\n",
    "\n",
    "            \n",
    "            conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "            val_conf_matrices.append(conf_matrix)\n",
    "\n",
    "            if epoch_acc > best_val_accuracy:\n",
    "                best_val_accuracy = epoch_acc\n",
    "                best_epoch = epoch + 1\n",
    "                early_stopping_counter = 0\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'accuracy': best_val_accuracy,\n",
    "                }, best_model_path)\n",
    "                print(f'Saved best model at epoch {epoch + 1} with validation accuracy: {best_val_accuracy:.4f}')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
    "        print(f'Best validation accuracy achieved at epoch {best_epoch}: {best_val_accuracy:.4f}')\n",
    "        break\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cd60b-7a05-45fc-808e-95d10c6154a7",
   "metadata": {},
   "source": [
    "### Save values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06109356-aa24-4cac-b7ed-26e1b551912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"epoch\": range(1, len(train_losses) + 1),  # Epochs start from 1\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_loss\": val_losses,\n",
    "    \"train_accuracy\": train_accuracies,\n",
    "    \"val_accuracy\": val_accuracies,\n",
    "    \"val_f1_score\": val_f1_scores,\n",
    "    \"val_recall\": val_recall,\n",
    "    \"val_precision\": val_precision,\n",
    "    \"val_conf_matrix\": val_conf_matrices,\n",
    "    \"val_labels\": val_labels, #\n",
    "    \"val_predictions\": val_predictions #\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to a CSV file\n",
    "file_name = f\"training_results_{current_time}.csv\"\n",
    "df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724e7e4-1123-491d-8eb5-19a24bef40df",
   "metadata": {},
   "source": [
    "# Visualization of training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc14ac-8896-4d2d-8490-f0a095a15ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c2ff8-e73b-4fc7-9aed-42cd84911cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "stopped_epoch = best_epoch + patience\n",
    "\n",
    "# Data from your training output\n",
    "epochs = range(1, stopped_epoch+1) ## stopped_epoch\n",
    "           \n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "fig.suptitle('Training and Validation Metrics', fontsize=16)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "ax1.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "ax1.set_title('Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_ylim(0, 3)\n",
    "ax1.set_xlim(1)\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax2.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "ax2.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
    "ax2.set_title('Model Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_xlim(1)\n",
    "ax2.set_xticks(ticks=np.arange(1, stopped_epoch + 1, 1)) # HARD CODED, epochs\n",
    "ax2.legend()\n",
    "ax2.grid()\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "fig.tight_layout()\n",
    "ax1.plot()\n",
    "ax2.plot()\n",
    "fig.show()\n",
    "\n",
    "# Calculate and print best validation accuracy\n",
    "best_val_accuracy = max(val_accuracies)\n",
    "best_epoch = val_accuracies.index(best_val_accuracy) + 1\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f} (Epoch {best_epoch})\")\n",
    "\n",
    "# Calculate and print final training-validation gap\n",
    "final_train_acc = train_accuracies[-1]\n",
    "final_val_acc = val_accuracies[-1]\n",
    "acc_gap = final_train_acc - final_val_acc\n",
    "print(f\"Final accuracy gap (train - val): {acc_gap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64da86-3eb4-48fa-94ab-e54a2ece09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1-Score\n",
    "f, ax = plt.subplots(figsize=(10, 5), sharex=True)\n",
    "ax.plot(val_f1_scores, label='Validation F1-Score', marker='o', color='blue')\n",
    "ax.set_xlabel('Epochs', loc='center')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('Validation F1-Score Over Epochs')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticks(ticks=np.arange(1, stopped_epoch + 1, 1))\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86513ba-7ae1-4232-bdd0-b3f56db277d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(best_epoch)\n",
    "# Confusion Matrix for the Best Epoch\n",
    "conf_matrix = val_conf_matrices[best_epoch - 1]  # Index corresponds to the best epoch\n",
    "class_names = sorted(['American Black Bear', 'California Ground Squirrel', 'Elk', 'Gray Fox', 'Red Deer', 'Unidentified Deer', 'Bobcat', 'Coyote', 'Empty', 'Mule Deer', 'Striped Skunk', 'Wild Boar'])\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 5), sharex=True)\n",
    "sns.heatmap(conf_matrix/ (len(val_dataset)/num_classes), annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title(f'Confusion Matrix (Epoch {best_epoch})')\n",
    "f.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99761675-b089-44a5-b147-a3c5513ff1bf",
   "metadata": {},
   "source": [
    "Test set contains 600 pictures per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c761c21-0b89-4fc9-8717-4921b9132f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(root='/exchange/dspro01/group3/data/test', transform=transform) # same transform as train and validation phases\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f320f9-202b-40c2-ae48-191210f9ca9c",
   "metadata": {},
   "source": [
    "## Load subset of test images for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590da83-a297-4fa0-9802-f98b66cddf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Create a subset of the first x images\n",
    "subset_indices = list(range(50))\n",
    "test_subset = Subset(test_dataset, subset_indices)\n",
    "\n",
    "# Create a DataLoader for the subset\n",
    "test_loader_subset = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4cc7e-7eaa-46e9-9bb7-531f4af1a628",
   "metadata": {},
   "source": [
    "## Load trained model and Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e192ac-370d-4a05-a553-bf173e1dddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "best_model_only = f'only_{best_model_path}'\n",
    "# Load the pre-trained ResNet model\n",
    "model = models.resnet50(pretrained=False)\n",
    "\n",
    "# Modify the 'fc' layer to have 12 output classes\n",
    "model.fc = nn.Linear(model.fc.in_features, 12)\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_only, map_location=torch.device('cpu')))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60f48c-3ec2-458e-8aa9-5d52f9c9b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted(['American Black Bear', 'California Ground Squirrel', \n",
    "               'Elk', 'Gray Fox', 'Red Deer', 'Unidentified Deer', \n",
    "               'Bobcat', 'Coyote', 'Empty', 'Mule Deer', \n",
    "               'Striped Skunk', 'Wild Boar'])\n",
    "class_mapping = {i: class_names[i] for i in range(12)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f78042-8fd9-491e-a8be-008044403b42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model on one picture of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba62bb-832a-46af-b983-01659396ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/exchange/dspro01/group3/data/test/bobcat/2015_Unit097_Ivan076_img0742.jpg'\n",
    "#image_path = '/exchange/dspro01/group3/data/test/gray fox/CA-01_08_13_2015_CA-01_0001873.jpg' #2nd example\n",
    "image = Image.open(image_path).convert('RGB') \n",
    "input_tensor = transform(image).unsqueeze(0) # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23a8fc-1154-4afc-b1fa-116f201bf9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    output = model(input_tensor)  # Forward pass\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Predicted class name: {class_mapping[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0b453-e759-4e72-81d0-d2df53caded7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model on all test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65741ba-f581-45d9-b302-694362261e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for inputs, labels in test_loader:\n",
    "        print('Loop start')\n",
    "        outputs = model(inputs)  \n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())  \n",
    "        print('label and prediction added to the lists')\n",
    "    print('Testing done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85f884-2709-4a11-9006-6b9cfc8eca50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save test data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43008b97-d5fb-4896-badc-56f507822f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "accuracy = np.mean(all_labels == all_predictions)\n",
    "print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "#Plot precision for some images\n",
    "for i in range(3):\n",
    "    print(f\"True label: {class_mapping[all_labels[i*600]]}, Predicted: {class_mapping[all_predictions[i*600]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf802da6-f75b-476c-a708-bdb2ec6a1e70",
   "metadata": {},
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86a567-c127-49f2-baef-d89347e025d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Safety check\n",
    "print(len(all_labels) == len(all_predictions))\n",
    "# Confusion Matrix for the Test set\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)  # Index corresponds to the best epoch\n",
    "class_names = sorted(['American Black Bear', 'California Ground Squirrel', 'Elk', 'Gray Fox', 'Red Deer', 'Unidentified Deer', 'Bobcat', 'Coyote', 'Empty', 'Mule Deer', 'Striped Skunk', 'Wild Boar'])\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 5), sharex=True)\n",
    "sns.heatmap(conf_matrix/ (len(all_labels)/num_classes), annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title(f'Confusion Matrix (Epoch {best_epoch})')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00780d8c-bfdf-4122-b1d5-4cc7fbf64809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Label': all_labels,\n",
    "    'Prediction': all_predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "filename = f'predictions_model_{best_model_only[-23:-4]}.csv'\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96a55c-3b04-4e87-a2d6-d473a9e7a3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7294964-b383-409f-8d35-5c29e217de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "best_epoch = 16 # Hard coded to avoir running the full script again\n",
    "\n",
    "class_names = sorted(['American Black Bear', 'California Ground Squirrel', 'Elk', 'Gray Fox', 'Red Deer', 'Unidentified Deer', 'Bobcat', 'Coyote', 'Empty', 'Mule Deer', 'Striped Skunk', 'Wild Boar'])\n",
    "f, ax = plt.subplots(figsize=(8, 5), sharex=True)\n",
    "# 480 = length validation set\n",
    "sns.heatmap((conf_matrix/480) , annot=True, fmt='.2f', cmap='mako_r', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title(f'Confusion Matrix (Epoch {best_epoch})')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70c09d-db31-48c1-b8be-a169694ef00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
